{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> __Johns Hopkins University__</h3>\n",
    "## <h3 align=\"center\">__Whiting School of Engineering__</h3>\n",
    "## <h3 align=\"center\">__Engineering for Professionals__</h3>\n",
    "## <h3 align=\"center\">__685.621 Algorithms for Data Science__</h3>\n",
    "## <h3 align=\"center\">__Homework 5__</h3>\n",
    "## <h3 align=\"center\">__Assigned at the start of Module 12__</h3>\n",
    "## <h3 align=\"center\">__Due at the end of Module 13__</h3><br>\n",
    "## <h3 align=\"center\">__Total Points 100/100__</h3>\n",
    "Class, the below is a standard set of instructions for each HW, in this assignment groups will be set up for collaboration.<br><br>\n",
    "Make sure your group starts one thread for the collaborative problems. You are required to participate in the collaborative problem and subproblem separately. Please do not directly post a complete\n",
    "solution, the goal is for the group to develop a solution after everyone has participated. Please ensure\n",
    "you have a write-up with solutions to each problem and subproblems, you are also required to submit\n",
    "code that will be compiled when grading the assignment. In each of the problems you are allowed to\n",
    "use built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __1 - Module 13 Note this is Collaborative Problem - Note: create threads for both subparts, you are required to participate in both subparts.__<br>\n",
    "*30 Points Total*<br><br>\n",
    "In this problem you will use a built-in Convolutional Neural Network (CNN) using either the Iris or numerical (MNIST) data sets and show the classification accuracy:<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "\n",
    "# load dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "# Split dataset into input (X) and output (y) variables\n",
    "X = dataset.iloc[:, 0:4].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "\n",
    "# Convert integers to one-hot encoding using a custom function\n",
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\"\"\"\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n",
    "y = to_categorical(encoded_Y)\n",
    "\n",
    "# Reshape the data to fit into a CNN. \n",
    "# Since we have 4 features, we'll treat each sample as a 1D \"image\" of size 4x1\n",
    "X = X.reshape(X.shape[0], 4, 1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 3s 104ms/step - loss: 0.9689 - accuracy: 0.7417 - val_loss: 0.8507 - val_accuracy: 0.8667\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.7708 - accuracy: 0.7917 - val_loss: 0.6650 - val_accuracy: 0.8333\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.6223 - accuracy: 0.7833 - val_loss: 0.5179 - val_accuracy: 0.8333\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.5071 - accuracy: 0.8167 - val_loss: 0.4151 - val_accuracy: 0.8667\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.4330 - accuracy: 0.8250 - val_loss: 0.3429 - val_accuracy: 0.9000\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 0.3825 - accuracy: 0.8250 - val_loss: 0.2988 - val_accuracy: 0.9000\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 0.3480 - accuracy: 0.8417 - val_loss: 0.2657 - val_accuracy: 0.9000\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3163 - accuracy: 0.8583 - val_loss: 0.2383 - val_accuracy: 0.9000\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.2910 - accuracy: 0.8750 - val_loss: 0.2138 - val_accuracy: 0.9333\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2675 - accuracy: 0.8833 - val_loss: 0.1953 - val_accuracy: 0.9333\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2398 - accuracy: 0.9417 - val_loss: 0.1703 - val_accuracy: 0.9667\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.2157 - accuracy: 0.9500 - val_loss: 0.1508 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.1938 - accuracy: 0.9583 - val_loss: 0.1399 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.1732 - accuracy: 0.9500 - val_loss: 0.1167 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.1549 - accuracy: 0.9583 - val_loss: 0.1148 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.1415 - accuracy: 0.9500 - val_loss: 0.1009 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.1263 - accuracy: 0.9583 - val_loss: 0.0941 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.1165 - accuracy: 0.9667 - val_loss: 0.0841 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.1095 - accuracy: 0.9667 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.1007 - accuracy: 0.9750 - val_loss: 0.0734 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0994 - accuracy: 0.9667 - val_loss: 0.0769 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0887 - accuracy: 0.9583 - val_loss: 0.0664 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 0.0944 - accuracy: 0.9667 - val_loss: 0.0623 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0799 - accuracy: 0.9750 - val_loss: 0.0668 - val_accuracy: 0.9667\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0789 - accuracy: 0.9667 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 0.0764 - accuracy: 0.9750 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.0725 - accuracy: 0.9750 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.0706 - accuracy: 0.9750 - val_loss: 0.0514 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 0.0548 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.0698 - accuracy: 0.9667 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.0669 - accuracy: 0.9750 - val_loss: 0.0506 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.0639 - accuracy: 0.9667 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0648 - accuracy: 0.9833 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.0627 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0658 - accuracy: 0.9667 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.0653 - accuracy: 0.9667 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0594 - accuracy: 0.9667 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.0558 - accuracy: 0.9750 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.0548 - accuracy: 0.9750 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0537 - accuracy: 0.9750 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 0.0562 - accuracy: 0.9667 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0571 - accuracy: 0.9750 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0536 - accuracy: 0.9750 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0521 - accuracy: 0.9750 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0508 - accuracy: 0.9833 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0507 - accuracy: 0.9750 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0485 - accuracy: 0.9833 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 38ms/step - loss: 0.0503 - accuracy: 0.9833 - val_loss: 0.0288 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28189408d60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Type the code for part 1 here ##\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(4, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 349ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "## Type the code for part 2 here ##\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Predict the values\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=encoder.classes_))\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = (y_pred_classes == y_true_classes).mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly, let's break this down:\n",
    "\n",
    "### Explanation ###\n",
    "\n",
    "\n",
    "### Part 1: Data Preprocessing\n",
    "\n",
    "1. **Import Necessary Libraries**: The code begins by importing various libraries such as pandas, scikit-learn, and numpy.\n",
    "   \n",
    "2. **Load Dataset**: The Iris dataset, which contains data about three species of the iris plant, is loaded from a file named \"iris.csv\" using pandas.\n",
    "   \n",
    "3. **Data Splitting**: The dataset is split into inputs (features) and outputs (labels). The features, denoted as \\( X \\), consist of the first four columns (such as sepal length, sepal width, petal length, and petal width). The labels, denoted as \\( y \\), are the species names (e.g., setosa, versicolor, virginica).\n",
    "\n",
    "4. **Data Normalization**: The features are standardized (or normalized) using the `StandardScaler` from scikit-learn. This ensures that all features have a mean of 0 and a standard deviation of 1, which can help in training neural networks.\n",
    "\n",
    "5. **Label Encoding**: The species names in \\( y \\) are transformed into integer values. For example, if the species are 'setosa', 'versicolor', and 'virginica', they might be encoded as 0, 1, and 2, respectively.\n",
    "\n",
    "6. **One-Hot Encoding**: These integer labels are then converted into a one-hot encoded format using a custom function named `to_categorical`. In this format, each label is represented as a binary vector. For instance, 'setosa' might be represented as [1, 0, 0], 'versicolor' as [0, 1, 0], and 'virginica' as [0, 0, 1].\n",
    "\n",
    "7. **Reshaping for CNN**: The feature data is reshaped to fit into a 1D convolutional neural network (CNN). Each sample is treated as a 1D \"image\" of size 4x1.\n",
    "\n",
    "8. **Train-Test Split**: The dataset is split into training and test sets, with 80% of the data used for training and 20% used for testing.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Neural Network Model\n",
    "\n",
    "1. **Model Definition**: A Keras sequential model is defined with the following layers:\n",
    "   - A 1D convolutional layer with 32 filters, kernel size of 2, and ReLU activation.\n",
    "   - A flatten layer to convert the 2D output of the previous layer to 1D.\n",
    "   - A dense (fully connected) layer with 50 nodes and ReLU activation.\n",
    "   - Another dense layer with 3 nodes (corresponding to the three iris species) and softmax activation, which will output the probabilities for each class.\n",
    "\n",
    "2. **Model Compilation**: The model is compiled using the Adam optimizer, with a loss function suitable for multi-class classification ('categorical_crossentropy'), and accuracy as the evaluation metric.\n",
    "\n",
    "3. **Model Training**: The model is trained using the training data for 50 epochs with a batch size of 10. Validation data (test set) is also provided to monitor the model's performance on unseen data during training.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 3: Evaluation\n",
    "\n",
    "1. **Predictions**: The trained model is used to make predictions on the test set. The output from the model is in the form of probabilities for each class. The class with the highest probability is chosen as the predicted class.\n",
    "\n",
    "2. **Classification Report**: A detailed classification report is printed, showing precision, recall, and f1-score for each class. This report indicates how well the model performed for each iris species.\n",
    "\n",
    "3. **Accuracy Calculation**: The overall accuracy of the model on the test set is calculated and printed. \n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "The output shows that the model achieved a perfect accuracy of 1.0 on the test set. The classification report confirms this by displaying precision, recall, and f1-score values of 1.00 for all three iris species. This indicates that the model was able to perfectly classify all samples in the test set, distinguishing between the three species without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __2 - Module 12 Optimization - Note this is a Collaborative Problem  __<br>\n",
    "70 points total\n",
    "\n",
    "In this problem, you will develop the Support Vector Machine (SVM) algorithm from scratch to classify the Iris data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [15 points] Using the SVM in the Optimization course notes, develop psuedocode for an SVM classifier using a linear and separately an rbf kernel.<br><br>\n",
    "2. [Optional] no need to discuss collaboratively - Analyze the runtime of your design in big O notation and calculate a total runtime such that each line of psuedocode is accounted for.<br><br>\n",
    "3. [55 points] Implement your SVM using Python:<br><br>\n",
    "    - Train three two class models using the Iris dataset as input training data, the Iris data will need to be reconfigured as a one vs. all or one vs. one data set.\n",
    "    - Process the test data set to determine which class each test observation belongs to, in this problem you will simply use all 150 observations as your test data.\n",
    "    - What is the classification accuracy of your design?\n",
    "    - Is there a difference in performance between the two kernels? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type the response for part 1 here ##\n",
    "\n",
    "1. Load the Iris dataset  \n",
    "\n",
    "Read the iris.csv file.  \n",
    "Extract the feature matrix X and species labels.  \n",
    "\n",
    "2. Reconfigure the dataset for one-vs-all classification  \n",
    "\n",
    "For each unique species in the dataset:  \n",
    "Set the label to +1 for rows matching the current species.  \n",
    "Set the label to -1 for all other rows.\n",
    "\n",
    "3. Define the Manual SVM classifier  \n",
    "\n",
    "Parameters:  \n",
    "kernel_type: Type of kernel to use ('linear' or 'rbf').  \n",
    "C: Regularization parameter.  \n",
    "epochs: Number of training iterations.  \n",
    "learning_rate: Learning rate for weight updates.  \n",
    "Methods:  \n",
    "kernel(x1, x2): Computes the kernel function.  \n",
    "If kernel_type is 'linear', use the dot product.\n",
    "$$\n",
    "K(x_1, x_2) = x_1^T x_2\n",
    "$$\n",
    "If kernel_type is 'rbf', use the Gaussian radial basis function:\n",
    "$$\n",
    "K(x_1, x_2) = \\exp\\left(-\\frac{\\lVert x_1 - x_2 \\rVert^2}{2 \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "    - `fit(X, y)`: Train the SVM classifier.  \n",
    "        - Initialize weights \\( \\alpha \\) to zeros.  \n",
    "        - For each epoch:  \n",
    "            - For each data point:  \n",
    "                - Update \\( \\alpha \\) based on whether the data point is misclassified or not.  \n",
    "                - Keep \\( \\alpha \\) bounded between 0 and \\( C \\).  \n",
    "        - Extract support vectors (data points with non-zero \\( \\alpha \\)).  \n",
    "        - Compute bias using the support vectors.  \n",
    "    - `predict(X)`: Classify input data.  \n",
    "        - For each data point, compute the sign of the decision function:  \n",
    "\n",
    "4. Train and evaluate the SVM classifier for each species  \n",
    "- For each species:    \n",
    "- Train the SVM classifier using both linear and RBF kernels.  \n",
    "- Predict the labels for the entire dataset.  \n",
    "- Compute the classification accuracy.  \n",
    "\n",
    "5. Output the classification accuracy for each species and kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Type the response for optional part 2 here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'setosa': 1.0,\n",
       "  'versicolor': 0.6866666666666666,\n",
       "  'virginica': 0.9666666666666667},\n",
       " {'setosa': 1.0,\n",
       "  'versicolor': 0.6866666666666666,\n",
       "  'virginica': 0.9666666666666667})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Type the code for part 3 here ##\n",
    "\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Load dataset\n",
    "iris = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "# Reconfigure the dataset using a one-vs-all approach\n",
    "def prepare_one_vs_all_data(target_species):\n",
    "    return iris['species'].apply(lambda x: 1 if x == target_species else -1).values\n",
    "\n",
    "species = iris['species'].unique()\n",
    "y_one_vs_all = {specie: prepare_one_vs_all_data(specie) for specie in species}\n",
    "X = iris.iloc[:, :-1].values\n",
    "\n",
    "def kernel(x1, x2):\n",
    "    # For a linear kernel\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def svm_train(X, y, C=1.0):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Calculate the Gram matrix\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K[i, j] = kernel(X[i], X[j])\n",
    "\n",
    "    # Setup the quadratic programming problem\n",
    "    # We'll use CVXOPT library as a solver\n",
    "    P = cvxopt.matrix(np.outer(y, y) * K)\n",
    "    q = cvxopt.matrix(-1 * np.ones(n_samples))\n",
    "    G = cvxopt.matrix(np.vstack((-1*np.eye(n_samples), np.eye(n_samples))))\n",
    "    h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))\n",
    "    A = cvxopt.matrix(y, (1, n_samples))\n",
    "    b = cvxopt.matrix(0.0)\n",
    "\n",
    "    # Solve the quadratic problem\n",
    "    solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    \n",
    "    # Extract the alphas\n",
    "    alphas = np.ravel(solution['x'])\n",
    "\n",
    "    # Get the support vectors\n",
    "    sv = alphas > 1e-5\n",
    "    support_vectors = X[sv]\n",
    "    support_alphas = alphas[sv]\n",
    "    support_targets = y[sv]\n",
    "    \n",
    "    # Calculate the bias\n",
    "    bias = np.mean(support_targets - np.sum(K[sv][:, sv] * support_targets * support_alphas, axis=1))\n",
    "    \n",
    "    return support_vectors, support_alphas, support_targets, bias\n",
    "\n",
    "def svm_predict(X, support_vectors, support_alphas, support_targets, bias):\n",
    "    y_pred = []\n",
    "    for sample in X:\n",
    "        prediction = 0\n",
    "        for sv, alpha, target in zip(support_vectors, support_alphas, support_targets):\n",
    "            prediction += alpha * target * kernel(sample, sv)\n",
    "        prediction += bias\n",
    "        y_pred.append(np.sign(prediction))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "\n",
    "# Proceeding to Step 4: Train the SVM using both linear and RBF kernels.\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "linear_accuracies = {}\n",
    "rbf_accuracies = {}\n",
    "\n",
    "for specie in species:\n",
    "    y = y_one_vs_all[specie]\n",
    "    \n",
    "    # Linear kernel\n",
    "    K_linear = kernel(X, X, kernel_type='linear')  # Compute the kernel matrix for linear kernel\n",
    "    svm_linear = ManualSVM(kernel_type='linear')\n",
    "    svm_linear.fit(K_linear, y)\n",
    "    y_pred_linear = svm_predict(K_linear, svm_linear)\n",
    "    linear_accuracies[specie] = np.mean(y_pred_linear == y)\n",
    "    \n",
    "    # RBF kernel\n",
    "    K_rbf = kernel(X, X, kernel_type='rbf')  # Compute the kernel matrix for RBF kernel\n",
    "    svm_rbf = ManualSVM(kernel_type='rbf')\n",
    "    svm_rbf.fit(K_rbf, y)\n",
    "    y_pred_rbf = svm_predict(K_rbf, svm_rbf)\n",
    "    rbf_accuracies[specie] = np.mean(y_pred_rbf == y)\n",
    "\n",
    "linear_accuracies, rbf_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type your response for part 3 here\n",
    "\n",
    "The SVM classifier has been trained using both linear and RBF kernels for the one-vs-all approach. Here are the classification accuracies on the entire dataset for each species:  \n",
    "\n",
    "For the species 'setosa':  \n",
    "\n",
    "Linear Kernel Accuracy: 100%  \n",
    "RBF Kernel Accuracy: 100%  \n",
    "\n",
    "For the species 'versicolor':\n",
    "\n",
    "Linear Kernel Accuracy: 68.67%  \n",
    "RBF Kernel Accuracy: 68.67%  \n",
    "\n",
    "For the species 'virginica':  \n",
    "\n",
    "Linear Kernel Accuracy: 96.67%  \n",
    "RBF Kernel Accuracy: 96.67%  \n",
    "\n",
    "From the results, it's evident that the SVM classifier is perfectly classifying the 'setosa' species using both kernels.   For the other two species, while the accuracy is lower for 'versicolor', the 'virginica' species is classified with relatively high accuracy.  \n",
    "\n",
    "These results demonstrate that our manually implemented SVM classifier works reasonably well, especially considering its simplicity. Advanced optimization techniques and more sophisticated models could lead to even better performance.  \n",
    "\n",
    "Now, to summarize the results:  \n",
    "\n",
    "The 'setosa' species can be linearly separated from the other two species, which is evident from the 100% accuracy for both kernels.  \n",
    "'versicolor' and 'virginica' are more challenging to separate, leading to lower accuracy. However, the accuracy is still quite reasonable, especially for the 'virginica' species.  \n",
    "There's no noticeable difference in performance between the linear and RBF kernels based on this dataset and our simple SVM implementation. This could be attributed to the nature of the data and the simplicity of the model. In higher-dimensional datasets, kernel functions like RBF might show more noticeable benefits.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<br><br>\n",
    "[1] Charu C. Aggarwal, Neural Networks and Deep Learning, Springer 2018<br><br>\n",
    "[2] Ahmad Abdolsaheb, How to make your Tic Tac Toe game unbeatable by using the minimax algorithm,\n",
    "2020, https://www.freecodecamp.org/news/how-to-make-your-tic-tac-toe-game-unbeatable-byusing-\n",
    "the-minimax-algorithm-9d690bad4b37/<br><br>\n",
    "[3] Francois Chollet, Deep Learning with Python, Manning, 2018<br><br>\n",
    "[4] Stephen Cook, The Complexity of Theorem Proving Procedures, Proceedings of the third annual ACM symposium<br><br>\n",
    "on Theory of computing, pp. 151-158, 1971\n",
    "[5] Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016,\n",
    "https://www.deeplearningbook.org/<br><br>\n",
    "[6] Patric Honner (Contributing Columnist), Why Winning in Rock-Paper-Scissors (and in Life) Isn’t Everything,\n",
    "What does John Nash’s game theory equilibrium concept look like in Rock-Paper-Scissors?, an article in the\n",
    "online Quanta Magazine, April 2, 2018, https://www.quantamagazine.org/the-game-theory-math-behindrock-\n",
    "paper-scissors-20180402/<br><br>\n",
    "[7] Richard M. Karp, Reducibility Among Combinatorial Problems, In R. E. Miller and J. W. Thatcher (editors),\n",
    "Complexity of Computer Computations, New York: Plenum, pp. 85-103, 1972<br><br>\n",
    "[8] Stephen G. Nash and Ariela Sofer, Linear and Nonlinear Programming, McGraw-Hill, 1996<br><br>\n",
    "[9] Stuart Russell and Peter Norvig, Arti cial Intelligence a Modern Approach Fourth Edition, Pearson, 2020<br><br>\n",
    "[10] Sergios Theodoridis and Konstantinos Koutroumbas, Pattern Recognition Third Edition, San Diego, CA:\n",
    "Academic Press, 2006<br><br>\n",
    "[11] Thomas H. Cormen, Charles E. Leiserson, Ronal L. Rivest and Cli ord Stein, Introduction to Algorithms,\n",
    "3rd Edition, MIT Press, 2009<br><br>\n",
    "[12] David Zuckerman, NP-Complete Problems Have a Version That’s Hard to Approximate, IEEE, Proceedings\n",
    "of the Eighth Annual Structure in Complexity Theory Conference, pp. 305-312, 1993<br><br>\n",
    "[13] David Zuckerman, On Unapproximable Versions of NP-Complete Problems, SIAM Journal on Computing,\n",
    "Volume 25, Issue 6, pp. 1293-1304, 1996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
