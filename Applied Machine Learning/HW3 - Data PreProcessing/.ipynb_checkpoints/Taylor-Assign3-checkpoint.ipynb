{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54afbd95",
   "metadata": {},
   "source": [
    "#### Andrew Taylor\n",
    "#### EN 705.601\n",
    "#### Applied Machine Learning\n",
    "## Homework 3 - Resubmitted with Python"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image-4.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d3e05c7e",
   "metadata": {},
   "source": [
    "### Predicting Suicide Probabilities from the \"Suicide Rates Overview 1985 to 2016\" Data Set\n",
    "\n",
    "#### Question 1: Objectives\n",
    "\n",
    "Suicide is a very real phenomenon, and anything that can be done to prevent it is worthwhile. If there was a way to predict suicide, or establish a probability for suicide from some factors under study, the resulting information could be used for intervention or social planning. Assuming there are definite factors which lead to suicide, the event can be modeled and predicted with a machine learning model. It can be learned, and that will be the objective of this notebook. \n",
    "\n",
    "#### Question 2: The Problem\n",
    "\n",
    "What can be classified or predicted with this dataset? It consists of 27,820 observations, with 12 features: country, year, sex, age, number of suicides, population, suicides per 100k population, country and year concatenated, Human Development Index, GDP for the observation year, GDP per capita, and the generation of the cohort recorded. Because the groups delineated by the observations are at the country/year/sex/age group level, our classification or prediction would be about groups so defined. A regression could be carried out to predict either the total number of suicides or the suicide rates (per 100k). One-hot encoding would be used for categorical variables, and after a train-test split, we could train a regression model and evaluate the performance on the testing set using metrics such as Mean Absolute Error, Mean Squared Error and R-Squared. Because we cover regression in a later module, for this exercise I will focus on classification. \n",
    "\n",
    "What can classification of the data set show? We can create categories/levels of suicide rates, such as low, medium, and high, then predict which category a particular group falls into (country/year/sex/age groups). Another classification task we will try is to predict the generation most at risk in a given year based on other features. This notebook will attempt these two tasks. These are the formulations of our problems under study. \n",
    "\n",
    "Let's also note that an unsupervised approach could yield insights. For instance, clustering countries based on features like GDP, GDP per capita, and suicide rates might reveal groups of countries with similar economic and mental health profiles. We could also use unsupervised methods to create new features which might be useful for other tasks.\n",
    "\n",
    "#### Question 3: The Dependent Variable\n",
    "\n",
    "We are going to do two classifications. For the first, suicide rates, we will transform the continuous variable 'suicides per 100k pop' into a categorical variable to serve as our dependent variable. This will have three levels, low, medium, and high suicide rate. This will let us look at which features indicate which groups most at risk.\n",
    "\n",
    "For our second study, the dependent variable will be the 'generation' feature. The task would be predicting the generation most at risk in a given year based on other features. These are our dependent/target variables and the other features would represent independent variables for prediction.\n",
    "\n",
    "#### Question 4: Correlations and Variable Ranking\n",
    "\n",
    "Using Weka, it is easy to run a preliminary correlation matrix and see the correlation between the features and the dependent variables. For suicide rate, we already have a continuous variable, so we only need to define the bins for the levels of risk. However, for the generation question we need to one-hot encode. Let's look at suicide rates first:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Looking at the results above, we can see the top 5 features that correlated to the target variable are sex, total suicides for the group, age, generation and country. The *i>?* feature is actually country, for some reason Weka isn't displaying the column header correctly. Also, total suicides is a somewhat redundant feature, so the main features that display information we can use are sex, age (a proxy for generation), and country. HDI, the Human Development Index comes in close after country, and this would make sense because HDI is \"is a statistical composite index of life expectancy, education (mean years of schooling completed and expected years of schooling upon entering the education system), and per capita income indicators, which is used to rank countries into four tiers of human development.\", per Wikipedia. So, we will focus on HDI too. To get the true numbers, let's drop the other features and re-run the correlation matrix:\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Unfortunately Weka did not recalculate the correlations based on the reduced features, and I don't know why. But, eyeballing the proportions of the correlations reported we can see that sex is about 10 times more correlated with suicide rate than country, and 3 times as correlated as age. HDI rounds out the group and the year of the observation does not seem to matter.\n",
    "\n",
    "For the generational risk question, I dropped all features except suicide rate and total suicides, because that's what we are trying to understand. It doesn't help us to know if the country is correlated with the generation, for example. After discretizing the suicide rate into low, medium and high, we see this matrix:\n",
    "\n",
    "![image-4.png](attachment:image-4.png)\n",
    "\n",
    "Once again Weka doesn't seem to be dropping features correctly, but from the numbers that low and medium rates of suicide are most highly correlated to the generation. We'll see more when we run a classification.\n",
    "\n",
    "#### Question 5: Pre-processing\n",
    "\n",
    "From this exploration, and some common sense, we can remove some features and perform Sequential Backwards Selection on the features that matter most for both questions. Since the data set comes from Kaggle it is generally well-formed without blanks and missing values to impute. The major features to use for each question are:\n",
    "\n",
    "*Predicting Suicide Rates*\n",
    "1) age\n",
    "2) sex\n",
    "3) Country\n",
    "4) HDI\n",
    "\n",
    "*Predicting Generational Risk*\n",
    "1) suicide rate per 100k\n",
    "\n",
    "The other features are either derived features, or exhibit low correlation with the target variable. In the case of generational risk, the other features are irrelevant.\n",
    "\n",
    "\n",
    "To preprocess this dataset for machine learning classification with SVM and Random Forest, we'll follow these steps:\n",
    "\n",
    "1) Handle missing values.\n",
    "2) One-hot encode nominal variables (including 'age').\n",
    "3) Normalize numerical features.\n",
    "4) Convert 'gdp_for_year ($)' to a proper numerical format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d65fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Step 0: Load the data\n",
    "file_path = 'master.csv'  # Replace with the actual path to the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean column names by removing extra quotes and trimming spaces\n",
    "data.columns = data.columns.str.strip(\"' \").str.replace(\"'\", \"\")\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "# Identify columns with missing values and impute them\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "nom_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Identify numerical and nominal columns\n",
    "num_cols = ['year', 'suicides_no', 'population', 'suicides/100k pop', 'HDI for year', 'gdp_per_capita ($)']\n",
    "nom_cols = ['country', 'sex', 'age', 'country-year', 'generation']\n",
    "\n",
    "# Create separate imputers for numerical and nominal columns\n",
    "imputers = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_imputer, num_cols),\n",
    "        ('nom', nom_imputer, nom_cols)])\n",
    "\n",
    "# Apply imputers\n",
    "data_imputed = pd.DataFrame(imputers.fit_transform(data), columns=num_cols+nom_cols)\n",
    "data_imputed[num_cols] = data_imputed[num_cols].apply(pd.to_numeric)\n",
    "\n",
    "# Step 2: One-hot encode nominal variables\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "\n",
    "# Step 3: Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 4: Convert 'gdp_for_year ($)' to a proper numerical format\n",
    "data_imputed['gdp_for_year ($)'] = data['gdp_for_year ($)'].str.replace(',', '').astype(float)\n",
    "num_cols.append('gdp_for_year ($)')\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler, num_cols),\n",
    "        ('nom', one_hot_encoder, nom_cols)\n",
    "    ])\n",
    "\n",
    "# Apply the preprocessing pipeline to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data_imputed)\n",
    "\n",
    "# Retrieve feature names for one-hot encoded columns\n",
    "one_hot_feature_names = preprocessor.named_transformers_['nom'].get_feature_names_out(input_features=nom_cols)\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = num_cols + one_hot_feature_names.tolist()\n",
    "\n",
    "# Convert the preprocessed data back to a DataFrame for better readability\n",
    "data_preprocessed_df = pd.DataFrame(data_preprocessed, columns=all_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe79a7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Question 6: Classification Model\n",
    "\n",
    "For both studies, we're looking at classification problems. Let's define the classification tasks and propose prototype models for each:\n",
    "\n",
    "#### 1. Suicide Rate Classification:\n",
    "\n",
    "##### Problem Definition:\n",
    "Predict the risk category (Low, Medium, High) of suicide rates based on age, sex, country, and Human Development Index (HDI).\n",
    "\n",
    "##### Prototype Model:\n",
    "**Model:** Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel.\n",
    "- The RBF kernel can capture non-linear relationships, which might be present in this dataset.\n",
    "\n",
    "**Features:**\n",
    "- Age: Continuous or ordinal variable (could be binned into age groups).\n",
    "- Sex: Binary variable.\n",
    "- Country: One-hot encoded to represent each country as a binary vector.\n",
    "- HDI: Continuous variable representing the Human Development Index.\n",
    "\n",
    "**Target Variable:**\n",
    "- Risk category of suicide rates: Categorical (Low, Medium, High).\n",
    "\n",
    "#### 2. Generational Risk Classification:\n",
    "\n",
    "##### Problem Definition:\n",
    "Predict the generation most at risk based on the risk categories (Low, Medium, High) of suicide rates.\n",
    "\n",
    "##### Prototype Model:\n",
    "**Model:** Random Forest Classifier.\n",
    "- Given the categorical nature of the independent variables (one-hot encoded generations), an ensemble method like Random Forest can effectively handle this type of data.\n",
    "\n",
    "**Features:**\n",
    "- Risk categories: Three binary variables representing Low, Medium, and High risk.\n",
    "\n",
    "**Target Variable:**\n",
    "- Generation: One-hot encoded, each generation will be a binary column.\n",
    "\n",
    "#### Model Training and Evaluation:\n",
    "For both models, the dataset will be split into training and testing sets, using an 80-20 split. The models will be trained on the training set and evaluated on the test set.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **Accuracy:** Gives an overall idea of how often the classifier is correct.\n",
    "- **Precision, Recall, F1-score:** These metrics will provide a detailed performance analysis for each class, especially useful if there's a class imbalance.\n",
    "- **Confusion Matrix:** To visually understand the true positives, false positives, true negatives, and false negatives for each class.\n",
    "\n",
    "#### Model Optimization:\n",
    "1. **Hyperparameter Tuning:** Use grid search or random search to find the optimal parameters for each model.\n",
    "2. **Feature Importance:** Especially for the Random Forest model, understanding which features are most influential can help refine the model and potentially simplify it.\n",
    "3. **Cross-Validation:** Use k-fold cross-validation for a more robust evaluation of the model's performance.\n",
    "\n",
    "This is the format I will follow below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29345a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to categorize suicide rates\n",
    "def categorize_suicide_rate(rate):\n",
    "    if rate <= 10:\n",
    "        return 'Low'\n",
    "    elif 10 < rate <= 20:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# Add 'Risk Category' to data_imputed based on 'suicides/100k pop'\n",
    "data_imputed['Risk Category'] = data_imputed['suicides/100k pop'].apply(categorize_suicide_rate)\n",
    "\n",
    "\n",
    "# Check if 'Risk Category' exists in data_imputed\n",
    "'Risk Category' in data_imputed.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da785af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9759166067577283,\n",
       " '              precision    recall  f1-score   support\\n\\n        High       0.98      0.97      0.97      1128\\n         Low       0.98      0.99      0.99      3505\\n      Medium       0.94      0.91      0.93       931\\n\\n    accuracy                           0.98      5564\\n   macro avg       0.97      0.96      0.96      5564\\nweighted avg       0.98      0.98      0.98      5564\\n',\n",
       " array([[1095,    0,   33],\n",
       "        [   0, 3487,   18],\n",
       "        [  27,   56,  848]], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  # Importing SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Importing evaluation metrics\n",
    "\n",
    "# Add risk category to the preprocessed dataset\n",
    "# Try adding the 'Risk Category' column to data_preprocessed_df \n",
    "try:\n",
    "    data_preprocessed_df['Risk Category'] = data_imputed['Risk Category'].values\n",
    "    operation_status = \"Successfully added 'Risk Category' to data_preprocessed_df.\"\n",
    "except Exception as e:\n",
    "    operation_status = f\"An error occurred: {e}\"\n",
    "\n",
    "operation_status\n",
    "\n",
    "\n",
    "data_preprocessed_df['Risk Category'] = data_imputed['Risk Category']\n",
    "\n",
    "# Step 3: Feature Selection (Updated)\n",
    "# Since the features are already one-hot encoded and normalized, we just need to select them from data_preprocessed_df\n",
    "# Identify the selected feature columns from the preprocessed DataFrame\n",
    "selected_features_svm = [col for col in data_preprocessed_df.columns if col in all_feature_names]\n",
    "X_svm = data_preprocessed_df[selected_features_svm]\n",
    "y_svm = data_preprocessed_df['Risk Category']\n",
    "\n",
    "# Step 4: Model Training and Evaluation (Updated)\n",
    "# Split the data into training and test sets (80-20 split)\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_svm, y_svm, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM model with RBF kernel\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_model.predict(X_test_svm)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test_svm, y_pred_svm)\n",
    "classification_rep_svm = classification_report(y_test_svm, y_pred_svm)\n",
    "confusion_mat_svm = confusion_matrix(y_test_svm, y_pred_svm)\n",
    "\n",
    "accuracy_svm, classification_rep_svm, confusion_mat_svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028968dc",
   "metadata": {},
   "source": [
    "### SVM Conclusions Based on Suicide Risk Research Question:\n",
    "\n",
    "#### Research Question:\n",
    "The SVM model was designed to predict the risk category (Low, Medium, High) of suicide rates based on age, sex, country, and Human Development Index (HDI).\n",
    "\n",
    "#### Model Performance:\n",
    "- **Accuracy**: Approximately 97.59%\n",
    "- **Precision, Recall, F1-score**: High for all categories; particularly strong for \"Low\" risk category.\n",
    "- **Confusion Matrix**: Most entries lie along the diagonal, indicating a high number of true positives and true negatives.\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "1. **Highly Accurate**: With an accuracy of nearly 98%, the model is highly effective at predicting the risk categories for suicide rates based on the features selected.\n",
    "  \n",
    "2. **Strong Precision and Recall**: The model has high precision and recall, which suggests that it correctly identifies positive cases and also minimizes false negatives. \n",
    "  \n",
    "3. **Low False Positives and Negatives**: The confusion matrix shows that false positives and false negatives are low, which further attests to the model's reliability.\n",
    "\n",
    "### Critique:\n",
    "\n",
    "1. **Overfitting Risk**: With such a high accuracy, there is a risk of overfitting. The model may not generalize well to new, unseen data. Cross-validation can be used to mitigate this risk.\n",
    "  \n",
    "2. **Class Imbalance**: The model performs exceptionally well on the \"Low\" risk category, which might indicate a class imbalance in the dataset. The performance on \"Medium\" and \"High\" categories, while good, is not as strong.\n",
    "  \n",
    "3. **Interpretability**: SVM models, especially with non-linear kernels, can be difficult to interpret. This could be a drawback if understanding the feature importance is crucial for the research.\n",
    "  \n",
    "4. **Limited Feature Set**: The model currently uses age, sex, country, and HDI as predictors. The inclusion of additional variables like economic indicators or mental health statistics could potentially improve the model's predictive power.\n",
    "\n",
    "Overall, the model performs exceptionally well in predicting the risk categories of suicide rates. However, further validation is needed to ensure that it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6c3fe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation for Generational Risk Classification\n",
    "# Add the 'generation' column back to the preprocessed DataFrame\n",
    "data_preprocessed_df['generation'] = data_imputed['generation'].values\n",
    "\n",
    "# We'll use the 'Risk Category' created earlier as features for this model\n",
    "X_rf = pd.get_dummies(data_preprocessed_df['Risk Category'], prefix='Risk')\n",
    "y_rf = data_imputed['generation']  # Target is the 'generation' column from data_imputed\n",
    "\n",
    "# Update X_rf to include the 'generation' column\n",
    "X_rf['generation'] = data_preprocessed_df['generation'].values\n",
    "\n",
    "print('generation' in data_preprocessed_df.columns)\n",
    "print('generation' in X_rf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6b2f8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " '                 precision    recall  f1-score   support\\n\\n        Boomers       1.00      1.00      1.00       967\\nG.I. Generation       1.00      1.00      1.00       573\\n   Generation X       1.00      1.00      1.00      1322\\n   Generation Z       1.00      1.00      1.00       303\\n     Millenials       1.00      1.00      1.00      1144\\n         Silent       1.00      1.00      1.00      1255\\n\\n       accuracy                           1.00      5564\\n      macro avg       1.00      1.00      1.00      5564\\n   weighted avg       1.00      1.00      1.00      5564\\n',\n",
       " array([[ 967,    0,    0,    0,    0,    0],\n",
       "        [   0,  573,    0,    0,    0,    0],\n",
       "        [   0,    0, 1322,    0,    0,    0],\n",
       "        [   0,    0,    0,  303,    0,    0],\n",
       "        [   0,    0,    0,    0, 1144,    0],\n",
       "        [   0,    0,    0,    0,    0, 1255]], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # Importing RandomForestClassifier\n",
    "\n",
    "# Feature Engineering for Random Forest Model \n",
    "\n",
    "# Step 1: Interaction Terms\n",
    "# Create interaction terms between the risk categories (Low, Medium, High)\n",
    "X_rf['Risk_Low_Medium'] = X_rf['Risk_Low'] * X_rf['Risk_Medium']\n",
    "X_rf['Risk_Medium_High'] = X_rf['Risk_Medium'] * X_rf['Risk_High']\n",
    "X_rf['Risk_Low_High'] = X_rf['Risk_Low'] * X_rf['Risk_High']\n",
    "\n",
    "# Step 2: Split the data into training and test sets (80-20 split)\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Class Weights \n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = y_train_rf.value_counts().to_dict()\n",
    "for key in class_weights.keys():\n",
    "    class_weights[key] = 1 / class_weights[key]\n",
    "\n",
    "# One-hot encode the 'generation' column\n",
    "generation_dummies = pd.get_dummies(X_rf['generation'], prefix='generation')\n",
    "\n",
    "# Drop the original 'generation' column and add the one-hot encoded columns\n",
    "X_rf_engineered = X_rf.drop('generation', axis=1)\n",
    "X_rf_engineered = pd.concat([X_rf_engineered, generation_dummies], axis=1)\n",
    "\n",
    "# Step 4: Model Training and Evaluation\n",
    "\n",
    "# run the Random Forest model training with the engineered features\n",
    "\n",
    "# Split the engineered data into training and test sets (80-20 split)\n",
    "X_train_rf_engineered, X_test_rf_engineered, y_train_rf, y_test_rf = train_test_split(X_rf_engineered, y_rf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Classifier model with new features and class weights\n",
    "rf_model_engineered = RandomForestClassifier(\n",
    "    n_estimators=50, \n",
    "    max_depth=10, \n",
    "    min_samples_split=2,\n",
    "    class_weight=class_weights,\n",
    "    random_state=42)\n",
    "\n",
    "# Fit the engineered model to the training data\n",
    "rf_model_engineered.fit(X_train_rf_engineered, y_train_rf)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf_engineered = rf_model_engineered.predict(X_test_rf_engineered)\n",
    "\n",
    "# Evaluate the engineered model\n",
    "accuracy_rf_engineered = accuracy_score(y_test_rf, y_pred_rf_engineered)\n",
    "classification_rep_rf_engineered = classification_report(y_test_rf, y_pred_rf_engineered)\n",
    "confusion_mat_rf_engineered = confusion_matrix(y_test_rf, y_pred_rf_engineered)\n",
    "\n",
    "accuracy_rf_engineered, classification_rep_rf_engineered, confusion_mat_rf_engineered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dca166",
   "metadata": {},
   "source": [
    "Evaluation of Model Performance\n",
    "The Random Forest model for predicting the most at-risk generation based on the risk categories of suicide rates yielded an accuracy of 1.0. The precision, recall, and f1-score are also 1.0 for each class, as shown in the classification report. The confusion matrix confirms that there are no misclassifications.\n",
    "\n",
    "Observations:\n",
    "Perfect Score: The model's accuracy, precision, recall, and F1-score are all perfect (1.0). This is usually a red flag for overfitting or data leakage.\n",
    "\n",
    "Class Balance: Since the model is perfectly predicting each class, it suggests that the class weights and feature engineering did not negatively impact the model. However, the perfect score still raises questions.\n",
    "\n",
    "Critique:\n",
    "Overfitting/Data Leakage: A perfect score is usually a red flag and should be investigated for overfitting or data leakage.\n",
    "\n",
    "Feature Importance: Understanding what's driving this perfect score is crucial.\n",
    "\n",
    "Validation: The model should be validated on a completely independent dataset to truly gauge its performance.\n",
    "\n",
    "Given these observations and critiques, further investigation is required to ensure that the model's performance is genuine and not a result of overfitting or data leakage. Let's try K-fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1140edf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1.]), 1.0, 0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize and train the Random Forest Classifier model with new features and class weights\n",
    "rf_model_kfold = RandomForestClassifier(\n",
    "    n_estimators=50, \n",
    "    max_depth=10, \n",
    "    min_samples_split=2,\n",
    "    class_weight=class_weights,\n",
    "    random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation (k=5)\n",
    "cross_val_scores = cross_val_score(rf_model_kfold, X_rf_engineered, y_rf, cv=5, scoring='accuracy')\n",
    "\n",
    "cross_val_scores, cross_val_scores.mean(), cross_val_scores.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e1cd9",
   "metadata": {},
   "source": [
    "Analysis of K=5 K-fold cross validation:\n",
    "\n",
    "The accuracy is still 1.0 across all five folds indicating some data leakage possibly (where the model has access to data it shouldn't). I'll research next steps but these were my preliminary results. It seems the SVM did well, but the Random Forest was suspiciously accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en685621)",
   "language": "python",
   "name": "en685621"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
