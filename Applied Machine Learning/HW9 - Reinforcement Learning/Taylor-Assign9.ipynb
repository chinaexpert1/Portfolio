{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f8aefc",
   "metadata": {},
   "source": [
    "#### Andrew Taylor\n",
    "#### atayl136\n",
    "#### EN 705.601 Applied Machine Learning\n",
    "## Homework 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124a9bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIM Game code for reference\n",
    "\n",
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game()->list:\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(_st:list)->(int,int):\n",
    "    xored = _st[0] ^ _st[1] ^ _st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(_st)\n",
    "    for pile in range(3):\n",
    "        s = _st[pile] ^ xored\n",
    "        if s <= _st[pile]:\n",
    "            return _st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st:list)->(int,int):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move\n",
    "\n",
    "def nim_qlearner(_st:list)->(int,int):\n",
    "    global qtable\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    return move, pile  # action\n",
    "\n",
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(_a:str, _b:str):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[_a] if side == 'A' else Engines[_b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n:int, _a:str, _b:str)->(int,int):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for _ in range(_n):\n",
    "        wins[game(_a, _b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {_a:>8s}{wins['A']:5d}  {_b:>8s}{wins['B']:5d}\")\n",
    "    return wins['A'], wins['B']\n",
    "\n",
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n:int):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=np.float32)\n",
    "    # play _n games\n",
    "    for _ in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "\n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            \n",
    "            # Switch sides for play and learning\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r:float, _st1:list, move:int, pile:int, q_future_best:float):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef34e85",
   "metadata": {},
   "source": [
    "### Answers to Questions\n",
    "\n",
    "### 1. Describe the environment in the Nim learning model.\n",
    "\n",
    "In the given Nim learning model, the environment consists of three piles, each containing a random number of items between 1 and 10 at the beginning of the game. The state of the environment is represented by a list of three integers, each representing the number of items in one of the piles. The environment is responsible for keeping track of the state and informing the agent about it. It also checks for the game termination condition, which is when all the piles are empty.\n",
    "\n",
    "### 2. Describe the agent(s) in the Nim learning model. Is Guru an agent?\n",
    "\n",
    "There are three types of agents in the Nim learning model:\n",
    "\n",
    "1. **Random Agent (`nim_random`)**: This agent chooses a pile at random from the non-empty piles and removes a random number of items from it.\n",
    "   \n",
    "2. **Guru (`nim_guru`)**: This agent uses the mathematical solution to Nim, which involves XORing the number of items in each pile to determine the optimal move. If the XOR result is zero, it falls back to making a random move.\n",
    "\n",
    "3. **Q-Learner (`nim_qlearner`)**: This agent uses Q-learning to choose its actions. It either exploits the best known action from its Q-table or explores a random action if it encounters a previously unvisited state.\n",
    "\n",
    "Yes, Guru is also an agent. It has a strategy to interact with the environment, even though it's deterministic and based on mathematical principles rather than learning from experience.\n",
    "\n",
    "### 3. Describe the reward and penalty in the Nim learning model.\n",
    "\n",
    "In the Nim learning model, the reward structure is binary:\n",
    "\n",
    "- **Reward**: The Q-learner receives a reward of 100.0 when it wins the game, i.e., when it removes the last item from the last non-empty pile.\n",
    "  \n",
    "- **Penalty**: There doesn't appear to be an explicit penalty for losing in the given code, but implicitly the agent gets a future reward of 0 in losing states, which is relatively undesirable compared to the winning reward.\n",
    "\n",
    "The Q-value update function `qtable_update` uses these rewards to update the Q-table based on the Q-learning algorithm.\n",
    "\n",
    "### 4. How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total?\n",
    "\n",
    "The number of possible states can be calculated as follows:\n",
    "\n",
    "- Each pile can have between 0 and 10 items, which gives us 11 possible item counts per pile.\n",
    "  \n",
    "- There are 3 piles.\n",
    "\n",
    "So, the total number of possible states would be $(11 \\times 11 \\times 11)$.\n",
    "\n",
    "The total number of possible states in the Nim game, given a maximum of 10 items per pile and 3 piles in total, is 1,331.\n",
    "\n",
    "### 5. How many possible unique actions there could be for player 1 take their first action in a NIM game with 10 items per pile and 3 piles total?\n",
    "\n",
    "In the Nim game, an action consists of choosing a pile and then removing a certain number of items from it. \n",
    "\n",
    "For the first action, each of the 3 piles can be chosen. Each pile has 10 items, so a player can remove between 1 and 10 items from the chosen pile.\n",
    "\n",
    "The number of unique actions for the first action can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{Number of unique actions} = (\\text{Number of piles}) \\times (\\text{Number of items that can be removed from each pile})$$\n",
    "\n",
    "\n",
    "The total number of possible unique actions that player 1 could take for their first action in a Nim game, given a maximum of 10 items per pile and 3 piles in total, is 30.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06fe1b",
   "metadata": {},
   "source": [
    "### 6. Improving the Nim Game Learning Model\n",
    "\n",
    "1. **Penalizing Losses**: One way to improve learning is to introduce a penalty for losing. This can make the Q-learning agent more sensitive to actions that lead to losing states.\n",
    "   \n",
    "2. **Learning Rate Decay**: Decaying the learning rate (\\( \\alpha \\)) over time can help the Q-learning algorithm to converge more effectively.\n",
    "\n",
    "3. **Epsilon-Greedy Strategy**: Introduce an epsilon-greedy strategy to balance exploration and exploitation, rather than just relying on the max Q-value or a random action.\n",
    "\n",
    "### Why not use Guru in the Learning Module?\n",
    "\n",
    "Using the Guru player inside the learning module would essentially hard-code the optimal strategy into the Q-learning agent. This would defeat the purpose of reinforcement learning, which aims to learn the optimal strategy from experience rather than having it pre-programmed.\n",
    "\n",
    "### Coding the Solution\n",
    "\n",
    "First, let's modify the existing `nim_qlearn` and `qtable_update` functions to incorporate the above improvements. Then we'll test the modified Q-learner against the existing Random and Guru players.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb7790a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((710, 290), (22, 978))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "ITEMS_MX = 10\n",
    "Alpha = 1.0\n",
    "Gamma = 0.8\n",
    "Reward = 100.0\n",
    "Penalty = -100.0  # Penalty for losing\n",
    "epsilon = 0.1  # For epsilon-greedy strategy\n",
    "\n",
    "# Initialize Q-table\n",
    "qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=np.float32)\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1, ITEMS_MX), randint(1, ITEMS_MX), randint(1, ITEMS_MX)]\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i] > 0])\n",
    "    return randint(1, _st[pile]), pile\n",
    "\n",
    "# Guru Nim player\n",
    "def nim_guru(_st):\n",
    "    xored = _st[0] ^ _st[1] ^ _st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(_st)\n",
    "    for pile in range(3):\n",
    "        s = _st[pile] ^ xored\n",
    "        if s <= _st[pile]:\n",
    "            return _st[pile] - s, pile\n",
    "\n",
    "# Enhanced Q-Learner\n",
    "def nim_qlearner_enhanced(_st):\n",
    "    global qtable\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Exploration\n",
    "        return nim_random(_st)\n",
    "    else:\n",
    "        # Exploitation\n",
    "        a = np.argmax(qtable[_st[0], _st[1], _st[2]])\n",
    "        move, pile = a % ITEMS_MX + 1, a // ITEMS_MX\n",
    "        if move <= 0 or _st[pile] < move:\n",
    "            return nim_random(_st)\n",
    "        return move, pile\n",
    "\n",
    "# Update Q-table\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile * ITEMS_MX + move - 1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)\n",
    "\n",
    "# Learning function\n",
    "def nim_qlearn_enhanced(_n):\n",
    "    global qtable\n",
    "    for _ in range(_n):\n",
    "        st1 = init_game()\n",
    "        while True:\n",
    "            move, pile = nim_random(st1)  # Exploration\n",
    "            st2 = list(st1)\n",
    "            st2[pile] -= move\n",
    "            if st2 == [0, 0, 0]:\n",
    "                qtable_update(Reward, st1, move, pile, 0)\n",
    "                break\n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "\n",
    "Engines = {'Random': nim_random, 'Guru': nim_guru, 'Enhanced_Qlearner': nim_qlearner_enhanced}\n",
    "\n",
    "# Play the game\n",
    "def game(_a, _b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[_a] if side == 'A' else Engines[_b]\n",
    "        move, pile = engine(state)\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:\n",
    "            return side\n",
    "        side = 'B' if side == 'A' else 'A'\n",
    "\n",
    "# Play multiple games\n",
    "def play_games(_n, _a, _b):\n",
    "    wins = defaultdict(int)\n",
    "    for _ in range(_n):\n",
    "        wins[game(_a, _b)] += 1\n",
    "    return wins['A'], wins['B']\n",
    "\n",
    "# Train the enhanced Q-learner\n",
    "nim_qlearn_enhanced(5000)\n",
    "\n",
    "# Test against Random and Guru players\n",
    "wins_enhanced_vs_random = play_games(1000, 'Enhanced_Qlearner', 'Random')\n",
    "wins_enhanced_vs_guru = play_games(1000, 'Enhanced_Qlearner', 'Guru')\n",
    "\n",
    "wins_enhanced_vs_random, wins_enhanced_vs_guru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3973c2",
   "metadata": {},
   "source": [
    "### Performance Results\n",
    "\n",
    "After training the enhanced Q-learner, we played 1000 games against both the Random and Guru players. Here are the results:\n",
    "\n",
    "- **Enhanced Q-learner vs Random**: \n",
    "  - Enhanced Q-learner won 710 games\n",
    "  - Random won 290 games\n",
    "\n",
    "- **Enhanced Q-learner vs Guru**: \n",
    "  - Enhanced Q-learner won 22 games\n",
    "  - Guru won 978 games\n",
    "\n",
    "### Observations\n",
    "\n",
    "- The enhanced Q-learner performs well against the Random player, winning the majority of the games.\n",
    "  \n",
    "- Against the Guru player, the enhanced Q-learner still falls short, winning only 22 out of 1000 games. This is expected since the Guru player uses the optimal mathematical strategy for Nim.\n",
    "\n",
    "While we did introduce some improvements, they didn't seem to help, the Q-learner still has difficulty beating the Guru player, which employs the mathematically optimal strategy for Nim. However, it's worth noting that the Q-learner is not trained to beat Guru but rather to learn an optimal strategy from scratch. In different settings or more complex variations of Nim, a well-trained Q-learner might perform better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12c51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en685621)",
   "language": "python",
   "name": "en685621"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
