{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0debbe43",
   "metadata": {},
   "source": [
    "#### Andrew Taylor\n",
    "#### EN705.601.83 Applied Machine Learning\n",
    "#### September 8, 2023\n",
    "\n",
    "### Homework #2 Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d023258",
   "metadata": {},
   "source": [
    "#### Question #1: Classifiers\n",
    "\n",
    "Let's answer this question with some descriptions, then in another section I'll compare and contrast.\n",
    "\n",
    "#### Description of ML Techniques:\n",
    "\n",
    "**Perceptron:**\n",
    "The perceptron is one of the earliest and simplest artificial neural network architectures. It was introduced by Frank Rosenblatt in 1957. The perceptron consists of a single layer of neurons that make binary decisions. It takes a vector of inputs, multiplies them with its weights, sums the products, and then passes the sum through a step function (typically a unit step function) to produce an output of either 0 or 1.\n",
    "\n",
    "**Support Vector Machines (SVM):**\n",
    "SVM is a supervised machine learning algorithm used for classification or regression. Introduced in the 1990s, it works by finding the hyperplane that best divides a dataset into classes. The primary principle is to maximize the margin between the closest data points (support vectors) of two classes. SVMs can be linear or non-linear, depending on the kernel used.\n",
    "\n",
    "**Decision Tree:**\n",
    "A decision tree is a flowchart-like structure in which each internal node represents a feature(or attribute), each branch represents a decision rule, and each leaf node represents an outcome. The topmost node in a decision tree is known as the root node. It learns to partition based on the attribute value. Decision trees can be used for both classification and regression.\n",
    "\n",
    "**Random Forest:**\n",
    "Random Forest is an ensemble learning method that constructs a multitude of decision trees at training time and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is particularly effective in avoiding overfitting by training on various subsets of the data and averaging the results.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comparison and Contrast:\n",
    "\n",
    "1. **Nature**:\n",
    "   - *Perceptron:* Neural network-based.\n",
    "   - *SVM:* Margin-based classifier.\n",
    "   - *Decision Tree:* Rule-based.\n",
    "   - *Random Forest:* Ensemble of decision trees.\n",
    "\n",
    "2. **Model Complexity**:\n",
    "   - *Perceptron:* Simple with a single layer.\n",
    "   - *SVM:* Can be complex especially with non-linear kernels.\n",
    "   - *Decision Tree:* Complexity varies with depth and branching.\n",
    "   - *Random Forest:* More complex due to multiple trees.\n",
    "\n",
    "3. **Handling Non-linear Data**:\n",
    "   - *Perceptron:* Struggles with non-linearly separable data.\n",
    "   - *SVM:* Can handle non-linearity using kernels.\n",
    "   - *Decision Tree:* Can handle non-linear data inherently.\n",
    "   - *Random Forest:* Naturally handles non-linearity due to the ensemble nature.\n",
    "\n",
    "4. **Overfitting**:\n",
    "   - *Perceptron:* Prone to overfitting on non-linearly separable data.\n",
    "   - *SVM:* Less prone due to margin optimization, but choice of kernel and parameters matters.\n",
    "   - *Decision Tree:* Can easily overfit if not pruned.\n",
    "   - *Random Forest:* Reduces overfitting through ensemble learning.\n",
    "\n",
    "5. **Training Speed**:\n",
    "   - *Perceptron:* Fast as it is a simple model.\n",
    "   - *SVM:* Slower, especially for large datasets or with complex kernels.\n",
    "   - *Decision Tree:* Moderate, depends on the depth and branching.\n",
    "   - *Random Forest:* Slower due to multiple trees, but can be parallelized.\n",
    "\n",
    "6. **Interpretability**:\n",
    "   - *Perceptron:* Moderately interpretable due to weights.\n",
    "   - *SVM:* Less interpretable, especially with non-linear kernels.\n",
    "   - *Decision Tree:* Highly interpretable as rules can be visualized.\n",
    "   - *Random Forest:* Less interpretable than a single decision tree but provides feature importance.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's answer the specific questions:\n",
    "\n",
    "---\n",
    "\n",
    "**Optimization Problem and Cost Function:**\n",
    "\n",
    "1. **Perceptron:**\n",
    "   - **Optimization Problem:** Yes.\n",
    "   - **Cost Function:** Perceptron uses a simple misclassification rate. The algorithm tries to minimize the number of misclassified samples.\n",
    "   \n",
    "2. **Support Vector Machines (SVM):**\n",
    "   - **Optimization Problem:** Yes.\n",
    "   - **Cost Function:** SVM minimizes the hinge loss subject to margin constraints. The objective is to maximize the margin between the two classes.\n",
    "   \n",
    "3. **Decision Tree:**\n",
    "   - **Optimization Problem:** Yes.\n",
    "   - **Cost Function:** Decision trees don't have a traditional cost function like the above models. Instead, they use metrics like entropy, Gini impurity, or classification error to decide on splits.\n",
    "   \n",
    "4. **Random Forest:**\n",
    "   - **Optimization Problem:** Yes, but at the individual tree level.\n",
    "   - **Cost Function:** Like decision trees, random forests use metrics like entropy or Gini impurity for their individual trees.\n",
    "\n",
    "---\n",
    "\n",
    "**Speed, Strength, Robustness, and Statistical considerations:**\n",
    "\n",
    "1. **Perceptron:**\n",
    "   - **Speed:** Fast.\n",
    "   - **Strength:** Good for linearly separable data.\n",
    "   - **Robustness:** Sensitive to noisy data and outliers.\n",
    "   - **Statistical:** Prone to overfitting on non-linearly separable data.\n",
    "   \n",
    "2. **SVM:**\n",
    "   - **Speed:** Moderate to slow, depending on kernel and dataset size.\n",
    "   - **Strength:** Effective for both linear and certain non-linear patterns.\n",
    "   - **Robustness:** Robust against overfitting, especially in high-dimensional space.\n",
    "   - **Statistical:** Effective, but can be sensitive to the choice of kernel and parameters.\n",
    "   \n",
    "3. **Decision Tree:**\n",
    "   - **Speed:** Fast to moderate.\n",
    "   - **Strength:** Can capture non-linear relationships.\n",
    "   - **Robustness:** Prone to overfitting if not pruned.\n",
    "   - **Statistical:** Can be unstable, small changes in data can lead to different trees.\n",
    "   \n",
    "4. **Random Forest:**\n",
    "   - **Speed:** Slower due to multiple trees, but can be parallelized.\n",
    "   - **Strength:** Can capture complex patterns and relationships.\n",
    "   - **Robustness:** More robust against overfitting compared to individual decision trees.\n",
    "   - **Statistical:** Provides a measure of feature importance and reduces variance.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Type Classifier Naturally Uses:**\n",
    "\n",
    "1. **Perceptron:**\n",
    "   - Linear combinations of features.\n",
    "   \n",
    "2. **SVM:**\n",
    "   - Linear or non-linear transformations based on kernels.\n",
    "   \n",
    "3. **Decision Tree:**\n",
    "   - Uses features directly to make decisions based on entropy, Gini impurity, or classification error.\n",
    "   \n",
    "4. **Random Forest:**\n",
    "   - Uses features directly like decision trees but across multiple trees.\n",
    "\n",
    "---\n",
    "\n",
    "**Which One to Try First on a Dataset?**\n",
    "\n",
    "The choice of which model to try first on a dataset depends on the nature and size of the dataset, as well as the specific problem at hand. However, as a general guideline:\n",
    "\n",
    "- For linearly separable data, starting with a perceptron or linear SVM can be a good choice.\n",
    "- For datasets with complex non-linear patterns but not too large in size, SVM with non-linear kernels can be effective.\n",
    "- Decision trees can be a good starting point due to their interpretability and ability to handle non-linear data.\n",
    "- Random Forest is often a good default choice for many datasets due to its robustness and ability to handle both linear and non-linear patterns.\n",
    "\n",
    "---\n",
    "\n",
    "In conclusion, the ideal model often varies with the nature of the data and problem. It's beneficial to start with a simpler model to establish a baseline and then explore more complex models as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546b0cd",
   "metadata": {},
   "source": [
    "#### Question #2: Definitions of Feature Types\n",
    "\n",
    "##### 1. Numerical\n",
    "**Definition:** Numerical features represent measurable quantities and can take any value within a range. They can be further divided into continuous (can take any value in a range) and discrete (can only take certain specific values).\n",
    "\n",
    "**Example from Iris dataset:** \n",
    "- sepal length: This is a continuous numerical feature as it can take any value within a range to represent the length of the sepal in centimeters.\n",
    "\n",
    "##### 2. Nominal\n",
    "**Definition:** Nominal features are categorical features that donâ€™t have a natural order or ranking. They can take two or more categories, but there's no intrinsic ordering to the categories.\n",
    "\n",
    "**Example from Iris dataset:** \n",
    "- species: This is a nominal feature as it can take values like \"setosa\", \"versicolor\", or \"virginica\". There's no inherent order to these species names.\n",
    "\n",
    "##### 3. Date\n",
    "**Definition:** Date features represent specific days, months, years, or even timestamps. They can be used to track the progression of time.\n",
    "\n",
    "**Example from a Air Quality dataset:** \n",
    "- **Air Quality Dataset**: This dataset contains daily readings of the air quality values from 2004 to 2005. A feature like Date in this dataset would indicate the specific day when the air quality was recorded.\n",
    "\n",
    "##### 4. Text\n",
    "**Definition:** Text features consist of words, sentences, or paragraphs. These are typically unstructured and require special preprocessing techniques to extract meaningful information.\n",
    "\n",
    "**Example from Newsgroups dataset:** \n",
    "- **20 Newsgroups**: This is a dataset for text classification, containing newsgroup documents, organized into 20 different newsgroups. Each document is a collection of text, representing the content of a post or an article.\n",
    "\n",
    "##### 5. Image\n",
    "**Definition:** Image features are typically represented as matrices of pixel values. Each pixel can have one (for grayscale images) or multiple values (for color images).\n",
    "\n",
    "**Example from a CIFAR-10 dataset:** \n",
    "- **CIFAR-10**: This dataset consists of 60,000 32x32 color images in 10 different classes, representing objects like 'airplane', 'automobile', 'bird', etc. Each image is represented as a 3-dimensional array of pixel values (32x32 pixels and 3 channels for RGB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a057a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en685621)",
   "language": "python",
   "name": "en685621"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
